\cleardoublepage
\phantomsection
\addcontentsline{toc}{section}{Заключение}    % Добавляем его в оглавление
\section*{Заключение}                         % Заголовок

В данной работе рассматривалась проблема выбора достаточного размера выборки. Заключается она в том, чтобы определить необходимое число объектов для построения адекватной модели машинного обучения. Несмотря на то, что понятие достаточности зачастую является неформальным и эвристическим, существующие методы позволяют определять достаточное число объектов. Однако большинство из них либо не имеют строгих математических обоснований, либо применимы только для проверки некоторой статистической гипотезы о распределении параметров модели.

Для построения универсального и применимого на практике критерия достаточности в работе было проведено исследование:
\begin{enumerate}
    \item Значений функции правдоподобия на бутстрапированных подвыборках;
    \item Расстояния между апостериорными распределениями параметров модели на схожих подвыборках.
\end{enumerate}

Первая часть была посвящена выборочной оценке математического ожидания и дисперсии функции правдоподобия выборки. Значение полного правдоподобия в точке, полученной методом максимума правдоподобия на некоторой подвыборке, является случайной величиной, зависящей от использованной подвыборки. В настоящей работе был проведен анализ моментов этой случайной величины.

Второй подход, изученный в работе, отсылает к близости апостериорных распределений параметров модели, полученных на подвыборках, отличающихся на один объект. Эти распределения были сравнены между собой при помощи дивергенции Кульбака-Лейблера и функции близости s-score. 

В результате проведенного анализа были предложены четыре новых подхода к определению достаточного размера выборки: D-, M-, KL- и S-достаточность. Согласно каждому из них, достаточность определяется как близость к нулю или единице соответствующей функции. При этом, если задана некая модель, подход можно считать корректным, если в ней при стремлении размера выборки к бесконечности и соответствующая функция устремляется к своему предельному значению.

Была показана корректность предложенных определений при определенных условиях на используемую вероятностную модель. Кроме того, были доказаны теоремы о близости моментов и полных байесовских прогнозов в модели линейной регрессии с нормальным априорным распределением параметров. Для эмпирического подтверждения полученных теоретических результатов был проведен вычислительный эксперимент. Целью эксперимента стала проверка соответствующих сходимостей и сравнение предложенных подходов между собой.

Таким образом, результаты настоящей работы могут быть применимы и в практических задачах. Это возможно, в частности, потому, что подходы, использующие функцию правдоподобия, допускают ее замену на функцию ошибки на обучающей выборке, и это подтверждается численными экспериментами. Более того, в других двух подходах расстояние между распределениями может быть вычислено и сэмплированием, что позволит приспособить их к случаю произвольной модели. 

Тем не менее существенным ограничением рассматриваемых подходов является требуемая в доказательствах линейность модели. Учитывая популярность и постоянное развитие нейронных сетей, появляется естественное желание получить критерий достаточности и для таких моделей, существенно нелинейных. Дальнейшие планы развития настоящей работы направлены именно в сторону нейронных сетей.